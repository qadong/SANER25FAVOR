{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_ids 已成功保存为 valid_index.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9333"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dgl.data.utils import load_graphs\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "graphs, graph_labels = load_graphs('/home/hdd/qingao/graphs.bin')\n",
    "graphs_by_id = dict(zip(graph_labels[\"graph_ids\"].tolist(), graphs))\n",
    "\n",
    "graph_ids = graph_labels[\"graph_ids\"].tolist()\n",
    "df = pd.DataFrame(graph_ids, columns=[\"graph_id\"])\n",
    "\n",
    "# 保存为 valid_index.csv\n",
    "df.to_csv(\"valid_index.csv\", index=False)\n",
    "\n",
    "print(\"graph_ids 已成功保存为 valid_index.csv\")\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有图的平均结点数目: 201.24472302582234\n"
     ]
    }
   ],
   "source": [
    "num_nodes_list = [g.number_of_nodes() for g in graphs]\n",
    "\n",
    "# 计算平均结点数目\n",
    "average_num_nodes = sum(num_nodes_list) / len(num_nodes_list)\n",
    "\n",
    "print(f\"所有图的平均结点数目: {average_num_nodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as th\n",
    "from tqdm import tqdm\n",
    "\n",
    "allfeats = [\n",
    "    \"api\", \"datatype\", \"literal\", \"operator\",\n",
    "]\n",
    "\n",
    "def load_additional_features(graphs_by_id, feat_names, split=\"fixed\", sample_text=\"\"):\n",
    "    \"\"\"\n",
    "    加载并将特征添加到图中。\n",
    "    graphs_by_id: 图的字典，包含所有图对象\n",
    "    feat_names: 需要加载的特征列表\n",
    "    split: 数据集分割标志\n",
    "    sample_text: 样本标志\n",
    "    \"\"\"\n",
    "    for feat in feat_names:\n",
    "        # 构造CSV文件路径\n",
    "        prefix = \"_ABS_DATAFLOW_\"\n",
    "        filepath = f\"/home/l1/qingao/DeepDFA/DDFA/storage/processed/bigvul/nodes_feat_{prefix}{feat}_all_limitall_10000_limitsubkeys_10000_{split}{sample_text}.csv\"\n",
    "        # 读取特征数据\n",
    "        feat_df = pd.read_csv(filepath, index_col=0)\n",
    "        \n",
    "        # 将特征添加到对应的图中\n",
    "        for graph_id, group in tqdm(feat_df.groupby(\"graph_id\"), f\"Adding feature {feat}\"):\n",
    "            if graph_id in graphs_by_id:\n",
    "                g = graphs_by_id[graph_id]\n",
    "                # 假设列名以 _ABS_DATAFLOW_{feat} 开头\n",
    "                feat_column = next(c for c in feat_df.columns if c.startswith(f\"_ABS_DATAFLOW_{feat}\"))\n",
    "                # 将特征转换为 tensor 并加入图的 ndata\n",
    "                g.ndata[f\"_ABS_DATAFLOW_{feat}\"] = th.LongTensor(group[feat_column].tolist())\n",
    "                \n",
    "    return graphs_by_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding feature api: 100%|██████████| 187062/187062 [00:06<00:00, 30368.57it/s]\n",
      "Adding feature datatype: 100%|██████████| 187062/187062 [00:04<00:00, 45371.33it/s]\n",
      "Adding feature literal: 100%|██████████| 187062/187062 [00:04<00:00, 44971.72it/s]\n",
      "Adding feature operator: 100%|██████████| 187062/187062 [00:04<00:00, 39027.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['feat', '_ABS_DATAFLOW_api', '_ABS_DATAFLOW_datatype', '_ABS_DATAFLOW_literal', '_ABS_DATAFLOW_operator'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 假设你已经有了 graphs_by_id 的图字典\n",
    "graphs_by_id = load_additional_features(graphs_by_id, allfeats)\n",
    "\n",
    "# 检查特征是否成功添加\n",
    "graph_id = list(graphs_by_id.keys())[0]  # 检查第一个图\n",
    "g = graphs_by_id[graph_id]\n",
    "print(g.ndata.keys())  # 打印图中已有的特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9333\n"
     ]
    }
   ],
   "source": [
    "print(len(graphs_by_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/l1/qingao/DeepDFA\")\n",
    "import itertools\n",
    "from dgl.nn.pytorch import GatedGraphConv, GlobalAttentionPooling\n",
    "import torch\n",
    "from torch import nn\n",
    "from DDFA.code_gnn.models.base_module import BaseModule\n",
    "from pytorch_lightning.utilities.cli import MODEL_REGISTRY\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "allfeats = [\n",
    "    \"api\", \"datatype\", \"literal\", \"operator\"\n",
    "]\n",
    "\n",
    "@MODEL_REGISTRY\n",
    "class FlowGNNGGNNModule(BaseModule):\n",
    "    def __init__(self,\n",
    "                 feat,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 n_steps,\n",
    "                 num_output_layers,\n",
    "                 label_style=\"graph\",\n",
    "                 concat_all_absdf=False,\n",
    "                 encoder_mode=False,\n",
    "                 code_embedding_dim=768,  # CodeT5 embedding dimension\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if \"_ABS_DATAFLOW\" in feat:\n",
    "            feat = \"_ABS_DATAFLOW\"\n",
    "        self.feature_keys = {\n",
    "            \"feature\": feat,\n",
    "        }\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.concat_all_absdf = concat_all_absdf\n",
    "\n",
    "        # Feature extractors\n",
    "        embedding_dim = hidden_dim\n",
    "        if self.concat_all_absdf:\n",
    "            self.all_embeddings = nn.ModuleDict({\n",
    "                of: nn.Embedding(input_dim, embedding_dim) for of in allfeats\n",
    "            })\n",
    "            embedding_dim *= len(allfeats)\n",
    "            hidden_dim *= len(allfeats)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "        # Graph stage\n",
    "        self.ggnn = GatedGraphConv(in_feats=embedding_dim,\n",
    "                                   out_feats=hidden_dim,\n",
    "                                   n_steps=n_steps,\n",
    "                                   n_etypes=1)\n",
    "\n",
    "        # CodeT5 integration\n",
    "        self.code_embedding_dim = code_embedding_dim\n",
    "\n",
    "        # Token-level aggregation (e.g., mean-pooling)\n",
    "        self.token_aggregation = nn.GRU(code_embedding_dim, 512, batch_first=True)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, graph, extrafeats):\n",
    "        \"\"\"\n",
    "        graph: DGL graph object\n",
    "        extrafeats: extra features (node-related features)\n",
    "        code_embeddings: precomputed embeddings from CodeT5 for the 'code' in nodes\n",
    "        \"\"\"\n",
    "        # 获取图中每个节点的 code embeddings\n",
    "        code_embeddings = graph.ndata['feat']  # 假设这已经是 [node_num, seq_len, hid_dim] 的形状\n",
    "\n",
    "        # 获取特征的嵌入\n",
    "        if self.concat_all_absdf:\n",
    "            cfeats = []\n",
    "            for otherfeat in allfeats:\n",
    "                feat = graph.ndata[f\"_ABS_DATAFLOW_{otherfeat}\"]\n",
    "                cfeats.append(self.all_embeddings[otherfeat](feat))\n",
    "            feat_embed = torch.cat(cfeats, dim=1)  # 特征嵌入\n",
    "        else:\n",
    "            feat = graph.ndata[self.feature_keys[\"feature\"]]\n",
    "            feat_embed = self.embedding(feat)\n",
    "\n",
    "        # Graph learning stage (GGNN)\n",
    "        ggnn_out = self.ggnn(graph, feat_embed)\n",
    "        # print(\"GGNN Output Size:\", ggnn_out.size())\n",
    "\n",
    "        # Token-level aggregation (reduce sequence dimension)\n",
    "        # 这里假设 code_embeddings 是 [node_num, seq_len, hid_dim]\n",
    "        # 我们需要将其转换为适合 GRU 的形状\n",
    "        code_embeddings = code_embeddings.view(-1, code_embeddings.size(1), self.code_embedding_dim)  # [node_num, seq_len, hid_dim]\n",
    "\n",
    "        # 进行 GRU 聚合\n",
    "        output, code_embeddings_agg = self.token_aggregation(code_embeddings)\n",
    "\n",
    "        # Expand aggregated CodeT5 embeddings to match GGNN output size\n",
    "        code_embeddings = code_embeddings_agg.squeeze(0)\n",
    "        # print('code_embeddings',code_embeddings.size())\n",
    "\n",
    "        # Concatenate GGNN output, node features, and CodeT5 embeddings\n",
    "        out = torch.cat([ggnn_out, feat_embed,code_embeddings], dim=-1)\n",
    "        # If you're generating node-level outputs, just return out\n",
    "        logits = out  # Remove pooling if not needed\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1015421/1697295338.py:351: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/hdd/qingao/DeepDFA/CodeT5/saved_models/repair/codeT5/checkpoint-best-acc/pytorch_model.bin'))\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/l1/qingao/DeepDFA/CodeT5')\n",
    "from modeling_t5 import T5ForConditionalGeneration\n",
    "\n",
    "class FLOWT5(T5ForConditionalGeneration):\n",
    "    def __init__(self, config,flow_gnn):\n",
    "        super().__init__(config)\n",
    "        self.flow_gnn = flow_gnn\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def forward_(self, **kwargs):\n",
    "        if 'input_ids' in kwargs:\n",
    "            kwargs['input_ids'] = kwargs['input_ids'].view(kwargs['input_ids'].size(0), -1)\n",
    "        if 'attention_mask' in kwargs:\n",
    "            kwargs['attention_mask'] = kwargs['attention_mask'].view(kwargs['attention_mask'].size(0), -1)\n",
    "\n",
    "        return super(FLOWT5, self).forward(\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # We need to resize as B x (N * L) instead of (B * N) x L here\n",
    "    # because the T5 forward method uses the input tensors to infer\n",
    "    # dimensions used in the decoder.\n",
    "    # EncoderWrapper resizes the inputs as (B * N) x L.\n",
    "    def forward(self, input_ids=None, attention_mask=None, graph=None, **kwargs):\n",
    "    # 获取 ggnn_output\n",
    "        ggnn_output = None\n",
    "        padding_length = 200\n",
    "        if graph is not None:\n",
    "            ggnn_output = torch.zeros(input_ids.size(0), padding_length, 768)\n",
    "            num_nodes = []\n",
    "            i = 0\n",
    "            for g in graph:\n",
    "                \n",
    "                out = self.flow_gnn(g, {})  # ggnn_output 的形状为 (batch_size,seq, 512)\n",
    "                if out.size(0)<padding_length:\n",
    "                    ggnn_output[i, :out.size(0)] = out \n",
    "                else:\n",
    "                    ggnn_output[i, :, :] = out[:padding_length,:] \n",
    "                num_nodes.append(out.size(0))\n",
    "                i +=  1\n",
    "        self.encoder.gnn_out = ggnn_output\n",
    "\n",
    "        # print(ggnn_output.size())\n",
    "        # 保证 input_ids 是 2D tensor，确保其类型为 LongTensor\n",
    "        if input_ids is not None:\n",
    "            # 如果 input_ids 是 3D 维度，需要展平为 2D\n",
    "            if input_ids.dim() == 3:\n",
    "                self.encoder.n_passages = input_ids.size(1)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1).long()  # 确保 input_ids 是 LongTensor\n",
    "            if ggnn_output is not None:\n",
    "                padding = torch.zeros(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "\n",
    "        # 在最后一个维度上拼接 input_ids 和 padding\n",
    "    \n",
    "                input_ids = torch.cat((input_ids, padding), dim=1)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.view(attention_mask.size(0), -1)  # 确保 attention_mask 也是 2D\n",
    "            if ggnn_output is not None:\n",
    "                        # 创建与 input_ids 相同大小的填充\n",
    "                padding = torch.ones(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "                for i, num in enumerate(num_nodes):\n",
    "                    # 将前 num 个元素设置为 1\n",
    "                    padding[i, :num] = 1\n",
    "                # 在最后一个维度上拼接 attention_mask 和 padding\n",
    "                attention_mask = torch.cat((attention_mask, padding), dim=1)\n",
    "\n",
    "        # 将 ggnn_output 通过 kwargs 传递到 EncoderWrapper 中处理\n",
    "        # print('input_ids',input_ids.size())\n",
    "        # print('attention_mask',attention_mask.size())\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, # 将 ggnn_output 传入到 EncoderWrapper 进行后续处理\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # We need to resize the inputs here, as the generate method expect 2D tensors\n",
    "    def generate(self, input_ids, attention_mask, max_length, graph,**kwargs):\n",
    "        self.encoder.n_passages = input_ids.size(1)\n",
    "        ggnn_output = None\n",
    "        padding_length = 200\n",
    "        if graph is not None:\n",
    "            ggnn_output = torch.zeros(input_ids.size(0), padding_length, 768)\n",
    "            num_nodes = []\n",
    "            i = 0\n",
    "            for g in graph:\n",
    "                \n",
    "                out = self.flow_gnn(g, {})  # ggnn_output 的形状为 (batch_size,seq, 512)\n",
    "                if out.size(0)<padding_length:\n",
    "                    ggnn_output[i, :out.size(0)] = out \n",
    "                else:\n",
    "                    ggnn_output[i, :, :] = out[:padding_length,:] \n",
    "                num_nodes.append(out.size(0))\n",
    "                i += 1\n",
    "        self.encoder.gnn_out = ggnn_output\n",
    "        # 保证 input_ids 是 2D tensor，确保其类型为 LongTensor\n",
    "        if input_ids is not None:\n",
    "            # 如果 input_ids 是 3D 维度，需要展平为 2D\n",
    "            if input_ids.dim() == 3:\n",
    "                self.encoder.n_passages = input_ids.size(1)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1).long()  # 确保 input_ids 是 LongTensor\n",
    "            if ggnn_output is not None:\n",
    "                padding = torch.zeros(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "\n",
    "        # 在最后一个维度上拼接 input_ids 和 padding\n",
    "    \n",
    "                input_ids = torch.cat((input_ids, padding), dim=1)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.view(attention_mask.size(0), -1)  # 确保 attention_mask 也是 2D\n",
    "            if ggnn_output is not None:\n",
    "                        # 创建与 input_ids 相同大小的填充\n",
    "                padding = torch.ones(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "                for i, num in enumerate(num_nodes):\n",
    "                    # 将前 num 个元素设置为 1\n",
    "                    padding[i, :num] = 1\n",
    "                # 在最后一个维度上拼接 attention_mask 和 padding\n",
    "                attention_mask = torch.cat((attention_mask, padding), dim=1)\n",
    "        \n",
    "            # 将 ggnn_output 添加到 kwargs 中，以便在编码器中使用\n",
    "        return super().generate(\n",
    "            input_ids=input_ids.view(input_ids.size(0), -1),\n",
    "            attention_mask=attention_mask.view(attention_mask.size(0), -1),\n",
    "            max_length=max_length,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def wrap_encoder(self, use_checkpoint=False):\n",
    "        \"\"\"\n",
    "        Wrap T5 encoder to obtain a Fusion-in-Decoder model.\n",
    "        \"\"\"\n",
    "        self.encoder = EncoderWrapper(self.encoder, use_checkpoint=use_checkpoint)\n",
    "\n",
    "    def unwrap_encoder(self):\n",
    "        \"\"\"\n",
    "        Unwrap Fusion-in-Decoder encoder, useful to load T5 weights.\n",
    "        \"\"\"\n",
    "        self.encoder = self.encoder.encoder\n",
    "        block = []\n",
    "        for mod in self.encoder.block:\n",
    "            block.append(mod.module)\n",
    "        block = nn.ModuleList(block)\n",
    "        self.encoder.block = block\n",
    "\n",
    "    def load_t5(self, state_dict):\n",
    "        self.unwrap_encoder()\n",
    "        self.load_state_dict(state_dict,strict=False)\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def set_checkpoint(self, use_checkpoint):\n",
    "        \"\"\"\n",
    "        Enable or disable checkpointing in the encoder.\n",
    "        See https://pytorch.org/docs/stable/checkpoint.html\n",
    "        \"\"\"\n",
    "        for mod in self.encoder.encoder.block:\n",
    "            mod.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def reset_score_storage(self):\n",
    "        \"\"\"\n",
    "        Reset score storage, only used when cross-attention scores are saved\n",
    "        to train a retriever.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            mod.layer[1].EncDecAttention.score_storage = None\n",
    "\n",
    "    def get_crossattention_scores(self, context_mask):\n",
    "        \"\"\"\n",
    "        Cross-attention scores are aggregated to obtain a single scalar per\n",
    "        passage. This scalar can be seen as a similarity score between the\n",
    "        question and the input passage. It is obtained by averaging the\n",
    "        cross-attention scores obtained on the first decoded token over heads,\n",
    "        layers, and tokens of the input passage.\n",
    "\n",
    "        More details in Distilling Knowledge from Reader to Retriever:\n",
    "        https://arxiv.org/abs/2012.04584.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        n_passages = context_mask.size(1)\n",
    "        for mod in self.decoder.block:\n",
    "            scores.append(mod.layer[1].EncDecAttention.score_storage)\n",
    "        scores = torch.cat(scores, dim=2)\n",
    "        bsz, n_heads, n_layers, _ = scores.size()\n",
    "        # batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "        scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n",
    "        scores = scores.masked_fill(~context_mask[:, None, None], 0.)\n",
    "        scores = scores.sum(dim=[1, 2, 4])\n",
    "        ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads\n",
    "        scores = scores/ntokens\n",
    "        return scores\n",
    "\n",
    "    def overwrite_forward_crossattention(self):\n",
    "        \"\"\"\n",
    "        Replace cross-attention forward function, only used to save\n",
    "        cross-attention scores.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            attn = mod.layer[1].EncDecAttention\n",
    "            attn.forward = types.MethodType(cross_attention_forward, attn)\n",
    "\n",
    "class EncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.main_input_name = \"input_ids\"\n",
    "        # self.linear = nn.Linear(896, 768)  # 用于调整拼接后的 hidden state 大小\n",
    "        self.encoder = encoder\n",
    "        self.gnn_out = None\n",
    "        apply_checkpoint_wrapper(self.encoder, use_checkpoint)\n",
    "\n",
    "        # Query 和 Key 的线性变换，用于计算注意力权重\n",
    "        # self.query_layer = nn.Linear(768, 512)  # 用于对T5的hidden state进行投影\n",
    "        # self.key_layer = nn.Linear(768, 512)    # 用于对GNN的输出进行投影\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, ggnn_output=None, **kwargs):\n",
    "        # print(input_ids.size(),attention_mask.size())\n",
    "        outputs = self.encoder(input_ids[:,:512], attention_mask[:,:512], **kwargs)\n",
    "        # print('input_ids', input_ids.size())\n",
    "        if self.gnn_out is not None:\n",
    "            self.gnn_out = self.gnn_out.to('cuda:1')\n",
    "            encoder_hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_dim)\n",
    "            # print(encoder_hidden_states.size())\n",
    "            # ggnn_output 是 (batch_size, 768)，我们需要将其转换为 query，变为 (batch_size, 1, 768)\n",
    "\n",
    "            encoder_hidden_states = torch.cat((encoder_hidden_states, self.gnn_out), dim=1)  # 替换填充部分\n",
    "            # print(encoder_hidden_states.size())\n",
    "\n",
    "\n",
    "            # print('outputs.last_hidden_state',outputs.last_hidden_state)\n",
    "            outputs.last_hidden_state = encoder_hidden_states\n",
    "\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "class CheckpointWrapper(torch.nn.Module):\n",
    "    def __init__(self, module, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, position_bias, **kwargs):\n",
    "        if self.use_checkpoint and self.training:\n",
    "            kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "            def custom_forward(*inputs):\n",
    "                output = self.module(*inputs, **kwargs)\n",
    "                empty = torch.tensor(\n",
    "                    [],\n",
    "                    dtype=torch.float,\n",
    "                    device=output[0].device,\n",
    "                    requires_grad=True)\n",
    "                output = tuple(x if x is not None else empty for x in output)\n",
    "                return output\n",
    "\n",
    "            output = torch.utils.checkpoint.checkpoint(\n",
    "                custom_forward,\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                position_bias\n",
    "            )\n",
    "            output = tuple(x if x.size() != 0 else None for x in output)\n",
    "        else:\n",
    "            output = self.module(hidden_states, attention_mask, position_bias, **kwargs)\n",
    "        return output\n",
    "\n",
    "def apply_checkpoint_wrapper(t5stack, use_checkpoint):\n",
    "    block = []\n",
    "    for mod in t5stack.block:\n",
    "        wrapped_mod = CheckpointWrapper(mod, use_checkpoint)\n",
    "        block.append(wrapped_mod)\n",
    "    block = nn.ModuleList(block)\n",
    "    t5stack.block = block\n",
    "\n",
    "def cross_attention_forward(\n",
    "        self,\n",
    "        input,\n",
    "        mask=None,\n",
    "        kv=None,\n",
    "        position_bias=None,\n",
    "        past_key_value_state=None,\n",
    "        head_mask=None,\n",
    "        query_length=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "    assert(kv != None)\n",
    "    assert(head_mask == None)\n",
    "    assert(position_bias != None or self.has_relative_attention_bias)\n",
    "\n",
    "    bsz, qlen, dim = input.size()\n",
    "    n_heads, d_heads = self.n_heads, self.d_kv\n",
    "    klen = kv.size(1)\n",
    "\n",
    "    q = self.q(input).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "    if past_key_value_state == None:\n",
    "        k = self.k(kv).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "        v = self.v(kv).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "    else:\n",
    "        k, v = past_key_value_state\n",
    "\n",
    "    scores = torch.einsum(\"bnqd,bnkd->bnqk\", q, k)\n",
    "\n",
    "    if mask is not None:\n",
    "       scores += mask\n",
    "\n",
    "    if position_bias is None:\n",
    "        position_bias = self.compute_bias(qlen, klen)\n",
    "    scores += position_bias\n",
    "\n",
    "    if self.score_storage is None:\n",
    "        self.score_storage = scores\n",
    "\n",
    "    attn = F.softmax(scores.float(), dim=-1).type_as(scores)\n",
    "    attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "\n",
    "    output = torch.matmul(attn, v)\n",
    "    output = output.transpose(1, 2).contiguous().view(bsz, -1, self.inner_dim)\n",
    "    output = self.o(output)\n",
    "\n",
    "    if use_cache:\n",
    "        output = (output,) + ((k, v),)\n",
    "    else:\n",
    "        output = (output,) + (None,)\n",
    "\n",
    "    if output_attentions:\n",
    "        output = output + (attn,)\n",
    "\n",
    "    if self.has_relative_attention_bias:\n",
    "        output = output + (position_bias,)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import dgl\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '/home/hdd/qingao/cache/huggingface/transformers/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c'\n",
    "    )\n",
    "tokenizer.add_tokens([\"Vul_Start\",\"Vul_End\"])\n",
    "model = transformers.T5ForConditionalGeneration.from_pretrained(\n",
    "    '/home/hdd/qingao/cache/huggingface/transformers/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c'\n",
    "    )\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(torch.load('/home/hdd/qingao/DeepDFA/CodeT5/saved_models/repair/codeT5/checkpoint-best-acc/pytorch_model.bin'))\n",
    "config = model.config\n",
    "\n",
    "\n",
    "input_dim = 8\n",
    "feat = \"_ABS_DATAFLOW_datatype_all_limitall_1000_limitsubkeys_1000\"\n",
    "gtype = \"cfg\"\n",
    "label_style = \"graph\"\n",
    "dsname = \"bigvul\"\n",
    "node_type_feat = None\n",
    "concat_all_absdf = True\n",
    "hidden_dim = 32\n",
    "n_steps = 5\n",
    "num_output_layers = 3\n",
    "\n",
    "flowgnn_model = FlowGNNGGNNModule(\n",
    "    feat,\n",
    "    input_dim,\n",
    "    hidden_dim,\n",
    "    n_steps,\n",
    "    num_output_layers,\n",
    "    label_style=label_style,\n",
    "    # freeze_graph=False,\n",
    "    # append_dataflow=\"before_graph\",\n",
    "    # codebert_feat=None,\n",
    "    # doc2vec_feat=None,\n",
    "    # glove_feat=None,\n",
    "    # num_node_types=flowgnn_datamodule.num_node_types,\n",
    "    # node_type_feat=node_type_feat,\n",
    "    # just_codebert=False,\n",
    "    concat_all_absdf=concat_all_absdf,\n",
    "    # undersample_node_on_loss_factor=None,\n",
    "    # test_every=False,\n",
    "    # tune_nni=False,\n",
    "    # positive_weight=None,\n",
    "    encoder_mode=True,\n",
    ")\n",
    "\n",
    "\n",
    "flow_model = FLOWT5(config,flow_gnn=flowgnn_model)\n",
    "flow_model.load_t5(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FLOWT5(\n",
       "  (shared): Embedding(32102, 768)\n",
       "  (encoder): EncoderWrapper(\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32102, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x CheckpointWrapper(\n",
       "          (module): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32102, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32102, bias=False)\n",
       "  (flow_gnn): FlowGNNGGNNModule(\n",
       "    (train_metrics): MetricCollection(\n",
       "      (Accuracy): Accuracy()\n",
       "      (Precision): Precision()\n",
       "      (Recall): Recall()\n",
       "      (F1Score): F1Score(),\n",
       "      prefix=train_\n",
       "    )\n",
       "    (val_metrics): MetricCollection(\n",
       "      (Accuracy): Accuracy()\n",
       "      (Precision): Precision()\n",
       "      (Recall): Recall()\n",
       "      (F1Score): F1Score(),\n",
       "      prefix=val_\n",
       "    )\n",
       "    (test_metrics): MetricCollection(\n",
       "      (Accuracy): Accuracy()\n",
       "      (Precision): Precision()\n",
       "      (Recall): Recall()\n",
       "      (F1Score): F1Score(),\n",
       "      prefix=test_\n",
       "    )\n",
       "    (test_metrics_positive): MetricCollection(\n",
       "      (Accuracy): Accuracy()\n",
       "      (Precision): Precision()\n",
       "      (Recall): Recall()\n",
       "      (F1Score): F1Score(),\n",
       "      prefix=test_1_\n",
       "    )\n",
       "    (test_metrics_negative): MetricCollection(\n",
       "      (Accuracy): Accuracy()\n",
       "      (Precision): Precision()\n",
       "      (Recall): Recall()\n",
       "      (F1Score): F1Score(),\n",
       "      prefix=test_0_\n",
       "    )\n",
       "    (test_pr_curve): PrecisionRecallCurve()\n",
       "    (test_pr_curve_bin): BinnedPrecisionRecallCurve()\n",
       "    (test_preds): CatMetric()\n",
       "    (test_labels): CatMetric()\n",
       "    (test_confmat): ConfusionMatrix()\n",
       "    (label_proportion): ModuleDict(\n",
       "      (train_label_proportion): MeanMetric()\n",
       "      (val_label_proportion): MeanMetric()\n",
       "      (test_label_proportion): MeanMetric()\n",
       "    )\n",
       "    (prediction_proportion): ModuleDict(\n",
       "      (train_prediction_proportion): MeanMetric()\n",
       "      (val_prediction_proportion): MeanMetric()\n",
       "      (test_prediction_proportion): MeanMetric()\n",
       "    )\n",
       "    (label_proportion_cut): ModuleDict(\n",
       "      (train_label_proportion_cut): MeanMetric()\n",
       "      (val_label_proportion_cut): MeanMetric()\n",
       "      (test_label_proportion_cut): MeanMetric()\n",
       "    )\n",
       "    (prediction_proportion_cut): ModuleDict(\n",
       "      (train_prediction_proportion_cut): MeanMetric()\n",
       "      (val_prediction_proportion_cut): MeanMetric()\n",
       "      (test_prediction_proportion_cut): MeanMetric()\n",
       "    )\n",
       "    (train_portion_positive): MeanMetric()\n",
       "    (loss_fn): BCEWithLogitsLoss()\n",
       "    (all_embeddings): ModuleDict(\n",
       "      (api): Embedding(8, 32)\n",
       "      (datatype): Embedding(8, 32)\n",
       "      (literal): Embedding(8, 32)\n",
       "      (operator): Embedding(8, 32)\n",
       "    )\n",
       "    (ggnn): GatedGraphConv(\n",
       "      (linears): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (gru): GRUCell(128, 128)\n",
       "    )\n",
       "    (token_aggregation): GRU(768, 512, batch_first=True)\n",
       "    (pooling): GlobalAttentionPooling(\n",
       "      (gate_nn): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6548/6548 [00:21<00:00, 304.46 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:02<00:00, 308.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
    "\n",
    "# 读取训练数据\n",
    "train_data = []\n",
    "with open('/home/l1/qingao/DeepDFA/DDFA/storage/external/train_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "# 获取有效的图ID\n",
    "valid_index = graph_labels[\"graph_ids\"]\n",
    "\n",
    "val_data = []\n",
    "with open('/home/l1/qingao/DeepDFA/DDFA/storage/external/val_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        val_data.append(json.loads(line))\n",
    "\n",
    "# 获取有效的图ID\n",
    "valid_index = graph_labels[\"graph_ids\"]\n",
    "\n",
    "\n",
    "# 筛选出有效的数据\n",
    "filtered_data = [item for item in train_data if item.get('Unnamed: 0') in valid_index]\n",
    "filtered_val_data = [item for item in val_data if item.get('Unnamed: 0') in valid_index]\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(filtered_data)  # 将 filtered_data 转换为 DataFrame\n",
    "val_df = pd.DataFrame(filtered_val_data)  # 将 filtered_val_data 转换为 DataFrame\n",
    "\n",
    "# 将 Pandas DataFrame 转换为 Hugging Face 的 Dataset 类型\n",
    "train_Ds = Dataset.from_pandas(train_df)\n",
    "val_Ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 512\n",
    "    source = ''.join(example['source']) if isinstance(example['source'], list) else example['source']\n",
    "    target = ''.join(example['target']) if isinstance(example['target'], list) else example['target']\n",
    "    inputs = tokenizer(source, truncation=True, max_length=MAX_LENGTH, padding='max_length')\n",
    "    labels = tokenizer(target, truncation=True, max_length=MAX_LENGTH, padding='max_length')\n",
    "    index = example['Unnamed: 0']\n",
    "    return {\n",
    "        \"input_ids\": inputs['input_ids'],\n",
    "        \"attention_mask\": inputs['attention_mask'],\n",
    "        \"labels\": labels['input_ids'],\n",
    "        \"index\" : torch.tensor(index)\n",
    "    }\n",
    "\n",
    "train_ds = train_Ds.map(process_func, batched=False, remove_columns=train_Ds.column_names)\n",
    "val_ds = val_Ds.map(process_func, batched=False, remove_columns=val_Ds.column_names)\n",
    "# 输出筛选后的数据\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=default_data_collator, batch_size=16, pin_memory=True)\n",
    "eval_dataloader = DataLoader(val_ds, collate_fn=default_data_collator, batch_size=1, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6548"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   1%|          | 49/6548 [00:00<00:14, 455.06 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6548/6548 [00:21<00:00, 307.03 examples/s]\n",
      "Map: 100%|██████████| 903/903 [00:02<00:00, 328.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_Ds.map(process_func, batched=False, remove_columns=train_Ds.column_names)\n",
    "val_ds = val_Ds.map(process_func, batched=False, remove_columns=val_Ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=default_data_collator, batch_size=16, pin_memory=True)\n",
    "eval_dataloader = DataLoader(val_ds, collate_fn=default_data_collator, batch_size=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_tokens(tokens):\n",
    "    tokens = tokens.replace(\"<pad>\", \"\")\n",
    "    tokens = tokens.replace(\"<s>\", \"\")\n",
    "    tokens = tokens.replace(\"</s>\", \"\")\n",
    "    tokens = tokens.replace(' ','')\n",
    "    tokens = tokens.replace(\"</s>\", \"\")\n",
    "    tokens = tokens.replace(\"<start>\", \"\").replace('<end>','')\n",
    "    tokens = re.sub(r'\\s+', ' ', tokens)\n",
    "    tokens = tokens.strip()\n",
    "    return tokens\n",
    "def eval(model,eval_dataloader,graphs_by_id):\n",
    "\n",
    "    bar = tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"eval\")\n",
    "    model.to('cuda:1')\n",
    "    model.eval()\n",
    "    exmatch = 0\n",
    "    for step, batch in enumerate(bar):\n",
    "        input_ids = batch['input_ids'].to('cuda:1')\n",
    "        attention_mask = batch['attention_mask'].to('cuda:1')\n",
    "        labels = batch['labels'].to('cuda:1')\n",
    "        index = batch['index'].to('cuda:1')\n",
    "\n",
    "        index_list = index.tolist()\n",
    "\n",
    "        if graphs_by_id is None:\n",
    "            graphs = None\n",
    "        else:\n",
    "            graphs = [graphs_by_id[i].to('cuda:1') for i in index_list if i in graphs_by_id]\n",
    "\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                max_length = 128,\n",
    "                                graph=graphs if graphs else None)\n",
    "        for output,label in zip(outputs,labels):\n",
    "            if(clean_tokens(tokenizer.decode(output,skip_special_tokens=True))==clean_tokens(tokenizer.decode(label,skip_special_tokens=True))):\n",
    "                exmatch+=1\n",
    "                # print('FlowT5:',tokenizer.decode(output,skip_special_tokens=True))\n",
    "                # print('T5',tokenizer.decode(output_code,skip_special_tokens=True))\n",
    "                # print('answer',tokenizer.decode(label,skip_special_tokens=True))\n",
    "    return exmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0] Train loss 0.037: 100%|██████████| 410/410 [08:35<00:00,  1.26s/it]\n",
      "[1] Train loss 0.024: 100%|██████████| 410/410 [08:52<00:00,  1.30s/it]\n",
      "[2] Train loss 0.022: 100%|██████████| 410/410 [08:50<00:00,  1.29s/it]\n",
      "[3] Train loss 0.021: 100%|██████████| 410/410 [08:50<00:00,  1.29s/it]\n",
      "[4] Train loss 0.02: 100%|██████████| 410/410 [08:49<00:00,  1.29s/it] \n",
      "[0] Train loss 0.023: 100%|██████████| 410/410 [08:17<00:00,  1.21s/it]\n",
      "eval: 100%|██████████| 903/903 [15:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "Save the best acc model into %s home/hdd/qingao/RMGArepair/saved_model/step5_nodes200_dec_enc_dec_gnnf_wd1e-4_2/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1] Train loss 0.027: 100%|██████████| 410/410 [08:01<00:00,  1.17s/it]\n",
      "eval: 100%|██████████| 903/903 [15:35<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2] Train loss 0.022: 100%|██████████| 410/410 [07:57<00:00,  1.16s/it]\n",
      "eval: 100%|██████████| 903/903 [16:08<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "Save the best acc model into %s home/hdd/qingao/RMGArepair/saved_model/step5_nodes200_dec_enc_dec_gnnf_wd1e-4_2/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3] Train loss 0.017: 100%|██████████| 410/410 [07:57<00:00,  1.16s/it]\n",
      "eval: 100%|██████████| 903/903 [15:01<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "Save the best acc model into %s home/hdd/qingao/RMGArepair/saved_model/step5_nodes200_dec_enc_dec_gnnf_wd1e-4_2/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4] Train loss 0.014: 100%|██████████| 410/410 [07:56<00:00,  1.16s/it]\n",
      "eval: 100%|██████████| 903/903 [15:11<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "Save the best acc model into %s home/hdd/qingao/RMGArepair/saved_model/step5_nodes200_dec_enc_dec_gnnf_wd1e-4_2/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5] Train loss 0.013: 100%|██████████| 410/410 [07:55<00:00,  1.16s/it]\n",
      "eval: 100%|██████████| 903/903 [14:35<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6] Train loss 0.013: 100%|██████████| 410/410 [07:56<00:00,  1.16s/it]\n",
      "eval: 100%|██████████| 903/903 [13:48<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7] Train loss 0.013: 100%|██████████| 410/410 [07:55<00:00,  1.16s/it]\n",
      "eval: 100%|██████████| 903/903 [13:50<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8] Train loss 0.013: 100%|██████████| 410/410 [07:55<00:00,  1.16s/it]\n",
      "eval: 100%|██████████| 903/903 [14:02<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9] Train loss 0.013: 100%|██████████| 410/410 [07:55<00:00,  1.16s/it]\n",
      "eval: 100%|██████████| 903/903 [13:47<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import os \n",
    "# # 冻结 CodeT5 的参数\n",
    "# for param in flow_model.shared.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in flow_model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 如果 GNN 部分是单独的模块，确保它的参数不被冻结\n",
    "# # 这里假设 GNN 部分的名称为 `gnn`，请根据实际情况修改\n",
    "# for param in flow_model.flow_gnn.parameters():\n",
    "#     param.requires_grad = False  # 确保 GNN 的参数可训练\n",
    "\n",
    "\n",
    "# for param in flow_model.decoder.parameters():  # 确保流入解码器的参数被冻结\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # 定义学习率和优化器\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-5\n",
    "num_epochs = 5\n",
    "best_em = 0\n",
    "# 冻结 CodeT5 encoder 和 decoder 的参数\n",
    "output_dir = 'home/hdd/qingao/RMGArepair/saved_model/step5_nodes200_dec_enc_dec_gnnf_wd1e-4_2/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for param in flow_model.flow_gnn.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in flow_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flow_model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义不进行权重衰减的参数\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 为优化器分组参数\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "     'weight_decay': 1e-4\n",
    "     },\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad], \n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 定义 AdamW 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "# for name, param in flow_model.flow_gnn.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=400,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps=args.warmup_steps,\n",
    "#                                             num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "# 训练循环\n",
    "for cur_epoch in range(0, num_epochs):\n",
    "    bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "    nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "    flow_model.to('cuda:1')\n",
    "    flow_model.train()\n",
    "    \n",
    "    for step, batch in enumerate(bar):\n",
    "        input_ids = batch['input_ids'].to('cuda:1')\n",
    "        attention_mask = batch['attention_mask'].to('cuda:1')\n",
    "        labels = batch['labels'].to('cuda:1')\n",
    "        index = batch['index'].to('cuda:1')\n",
    "\n",
    "        index_list = index.tolist()\n",
    "\n",
    "        if graphs_by_id is None:\n",
    "            graphs = None\n",
    "        else:\n",
    "            graphs = [graphs_by_id[i].to('cuda:1') for i in index_list if i in graphs_by_id]\n",
    "\n",
    "            outputs = flow_model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                 labels=labels,\n",
    "                                 graph=graphs if graphs else None)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        loss.backward()\n",
    "\n",
    "        if nb_tr_steps % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Flow GNN Parameters and Gradients:\")\n",
    "            # for name, param in flow_model.flow_gnn.named_parameters():\n",
    "            #     if param.requires_grad:\n",
    "            #         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "            # break\n",
    "            # 计算和打印平均损失\n",
    "            train_loss = round(tr_loss / nb_tr_steps, 4) if nb_tr_steps > 0 else 0\n",
    "            bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "    # em = eval(flow_model,eval_dataloader,graphs_by_id)\n",
    "    # print(em)\n",
    "    # if em>best_em:\n",
    "    #     best_em = em\n",
    "    #     output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "    #     torch.save(flow_model.state_dict(), output_model_file)\n",
    "    #     print(\"Save the best acc model into %s\", output_model_file)\n",
    "for param in flow_model.flow_gnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flow_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flow_model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义不进行权重衰减的参数\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 为优化器分组参数\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "     'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad], \n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "lr = 1e-4\n",
    "# 定义 AdamW 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "# for name, param in flow_model.flow_gnn.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=400,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps=args.warmup_steps,\n",
    "#                                             num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "# 训练循环\n",
    "for cur_epoch in range(0, 10):\n",
    "    bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "    nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "    flow_model.to('cuda:1')\n",
    "    flow_model.train()\n",
    "    \n",
    "    for step, batch in enumerate(bar):\n",
    "        input_ids = batch['input_ids'].to('cuda:1')\n",
    "        attention_mask = batch['attention_mask'].to('cuda:1')\n",
    "        labels = batch['labels'].to('cuda:1')\n",
    "        index = batch['index'].to('cuda:1')\n",
    "\n",
    "        index_list = index.tolist()\n",
    "\n",
    "        if graphs_by_id is None:\n",
    "            graphs = None\n",
    "        else:\n",
    "            graphs = [graphs_by_id[i].to('cuda:1') for i in index_list if i in graphs_by_id]\n",
    "\n",
    "            outputs = flow_model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                 labels=labels,\n",
    "                                 graph=graphs if graphs else None)\n",
    "            \n",
    "\n",
    "\n",
    "        loss = outputs.loss\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        loss.backward()\n",
    "\n",
    "        if nb_tr_steps % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Flow GNN Parameters and Gradients:\")\n",
    "            # for name, param in flow_model.flow_gnn.named_parameters():\n",
    "            #     if param.requires_grad:\n",
    "            #         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "            # break\n",
    "            # 计算和打印平均损失\n",
    "            train_loss = round(tr_loss / nb_tr_steps, 4) if nb_tr_steps > 0 else 0\n",
    "            bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "    em = eval(flow_model,eval_dataloader,graphs_by_id)\n",
    "    print(em)\n",
    "    if em>best_em:\n",
    "        best_em = em\n",
    "        output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "        torch.save(flow_model.state_dict(), output_model_file)\n",
    "        print(\"Save the best acc model into %s\", output_model_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_782618/3718301340.py:351: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/hdd/qingao/DeepDFA/CodeT5/saved_models/repair/codeT5/checkpoint-best-acc/pytorch_model.bin'))\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/l1/qingao/DeepDFA/CodeT5')\n",
    "from modeling_t5 import T5ForConditionalGeneration\n",
    "\n",
    "class FLOWT5(T5ForConditionalGeneration):\n",
    "    def __init__(self, config,flow_gnn):\n",
    "        super().__init__(config)\n",
    "        self.flow_gnn = flow_gnn\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def forward_(self, **kwargs):\n",
    "        if 'input_ids' in kwargs:\n",
    "            kwargs['input_ids'] = kwargs['input_ids'].view(kwargs['input_ids'].size(0), -1)\n",
    "        if 'attention_mask' in kwargs:\n",
    "            kwargs['attention_mask'] = kwargs['attention_mask'].view(kwargs['attention_mask'].size(0), -1)\n",
    "\n",
    "        return super(FLOWT5, self).forward(\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # We need to resize as B x (N * L) instead of (B * N) x L here\n",
    "    # because the T5 forward method uses the input tensors to infer\n",
    "    # dimensions used in the decoder.\n",
    "    # EncoderWrapper resizes the inputs as (B * N) x L.\n",
    "    def forward(self, input_ids=None, attention_mask=None, graph=None, **kwargs):\n",
    "    # 获取 ggnn_output\n",
    "        ggnn_output = None\n",
    "        padding_length = 200\n",
    "        if graph is not None:\n",
    "            ggnn_output = torch.zeros(input_ids.size(0), padding_length, 768)\n",
    "            num_nodes = []\n",
    "            i = 0\n",
    "            for g in graph:\n",
    "                \n",
    "                out = self.flow_gnn(g, {})  # ggnn_output 的形状为 (batch_size,seq, 512)\n",
    "                if out.size(0)<padding_length:\n",
    "                    ggnn_output[i, :out.size(0)] = out \n",
    "                else:\n",
    "                    ggnn_output[i, :, :] = out[:padding_length,:] \n",
    "                num_nodes.append(out.size(0))\n",
    "                i += 1\n",
    "        self.encoder.gnn_out = ggnn_output\n",
    "\n",
    "        # print(ggnn_output.size())\n",
    "        # 保证 input_ids 是 2D tensor，确保其类型为 LongTensor\n",
    "        if input_ids is not None:\n",
    "            # 如果 input_ids 是 3D 维度，需要展平为 2D\n",
    "            if input_ids.dim() == 3:\n",
    "                self.encoder.n_passages = input_ids.size(1)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1).long()  # 确保 input_ids 是 LongTensor\n",
    "            if ggnn_output is not None:\n",
    "                padding = torch.zeros(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "\n",
    "        # 在最后一个维度上拼接 input_ids 和 padding\n",
    "    \n",
    "                input_ids = torch.cat((input_ids, padding), dim=1)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.view(attention_mask.size(0), -1)  # 确保 attention_mask 也是 2D\n",
    "            if ggnn_output is not None:\n",
    "                        # 创建与 input_ids 相同大小的填充\n",
    "                padding = torch.ones(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "                for i, num in enumerate(num_nodes):\n",
    "                    # 将前 num 个元素设置为 1\n",
    "                    padding[i, :num] = 1\n",
    "                # 在最后一个维度上拼接 attention_mask 和 padding\n",
    "                attention_mask = torch.cat((attention_mask, padding), dim=1)\n",
    "\n",
    "        # 将 ggnn_output 通过 kwargs 传递到 EncoderWrapper 中处理\n",
    "        # print('input_ids',input_ids.size())\n",
    "        # print('attention_mask',attention_mask.size())\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, # 将 ggnn_output 传入到 EncoderWrapper 进行后续处理\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # We need to resize the inputs here, as the generate method expect 2D tensors\n",
    "    def generate(self, input_ids, attention_mask, max_length, graph,**kwargs):\n",
    "        self.encoder.n_passages = input_ids.size(1)\n",
    "        ggnn_output = None\n",
    "        padding_length = 200\n",
    "        if graph is not None:\n",
    "            ggnn_output = torch.zeros(input_ids.size(0), padding_length, 768)\n",
    "            num_nodes = []\n",
    "            i = 0\n",
    "            for g in graph:\n",
    "                \n",
    "                out = self.flow_gnn(g, {})  # ggnn_output 的形状为 (batch_size,seq, 512)\n",
    "                if out.size(0)<padding_length:\n",
    "                    ggnn_output[i, :out.size(0)] = out \n",
    "                else:\n",
    "                    ggnn_output[i, :, :] = out[:padding_length,:] \n",
    "                num_nodes.append(out.size(0))\n",
    "                i += 1\n",
    "        self.encoder.gnn_out = ggnn_output\n",
    "        # 保证 input_ids 是 2D tensor，确保其类型为 LongTensor\n",
    "        if input_ids is not None:\n",
    "            # 如果 input_ids 是 3D 维度，需要展平为 2D\n",
    "            if input_ids.dim() == 3:\n",
    "                self.encoder.n_passages = input_ids.size(1)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1).long()  # 确保 input_ids 是 LongTensor\n",
    "            if ggnn_output is not None:\n",
    "                padding = torch.zeros(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "\n",
    "        # 在最后一个维度上拼接 input_ids 和 padding\n",
    "    \n",
    "                input_ids = torch.cat((input_ids, padding), dim=1)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.view(attention_mask.size(0), -1)  # 确保 attention_mask 也是 2D\n",
    "            if ggnn_output is not None:\n",
    "                        # 创建与 input_ids 相同大小的填充\n",
    "                padding = torch.ones(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "                for i, num in enumerate(num_nodes):\n",
    "                    # 将前 num 个元素设置为 1\n",
    "                    padding[i, :num] = 1\n",
    "                # 在最后一个维度上拼接 attention_mask 和 padding\n",
    "                attention_mask = torch.cat((attention_mask, padding), dim=1)\n",
    "        \n",
    "            # 将 ggnn_output 添加到 kwargs 中，以便在编码器中使用\n",
    "        return super().generate(\n",
    "            input_ids=input_ids.view(input_ids.size(0), -1),\n",
    "            attention_mask=attention_mask.view(attention_mask.size(0), -1),\n",
    "            max_length=max_length,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def wrap_encoder(self, use_checkpoint=False):\n",
    "        \"\"\"\n",
    "        Wrap T5 encoder to obtain a Fusion-in-Decoder model.\n",
    "        \"\"\"\n",
    "        self.encoder = EncoderWrapper(self.encoder, use_checkpoint=use_checkpoint)\n",
    "\n",
    "    def unwrap_encoder(self):\n",
    "        \"\"\"\n",
    "        Unwrap Fusion-in-Decoder encoder, useful to load T5 weights.\n",
    "        \"\"\"\n",
    "        self.encoder = self.encoder.encoder\n",
    "        block = []\n",
    "        for mod in self.encoder.block:\n",
    "            block.append(mod.module)\n",
    "        block = nn.ModuleList(block)\n",
    "        self.encoder.block = block\n",
    "\n",
    "    def load_t5(self, state_dict):\n",
    "        self.unwrap_encoder()\n",
    "        self.load_state_dict(state_dict,strict=False)\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def set_checkpoint(self, use_checkpoint):\n",
    "        \"\"\"\n",
    "        Enable or disable checkpointing in the encoder.\n",
    "        See https://pytorch.org/docs/stable/checkpoint.html\n",
    "        \"\"\"\n",
    "        for mod in self.encoder.encoder.block:\n",
    "            mod.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def reset_score_storage(self):\n",
    "        \"\"\"\n",
    "        Reset score storage, only used when cross-attention scores are saved\n",
    "        to train a retriever.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            mod.layer[1].EncDecAttention.score_storage = None\n",
    "\n",
    "    def get_crossattention_scores(self, context_mask):\n",
    "        \"\"\"\n",
    "        Cross-attention scores are aggregated to obtain a single scalar per\n",
    "        passage. This scalar can be seen as a similarity score between the\n",
    "        question and the input passage. It is obtained by averaging the\n",
    "        cross-attention scores obtained on the first decoded token over heads,\n",
    "        layers, and tokens of the input passage.\n",
    "\n",
    "        More details in Distilling Knowledge from Reader to Retriever:\n",
    "        https://arxiv.org/abs/2012.04584.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        n_passages = context_mask.size(1)\n",
    "        for mod in self.decoder.block:\n",
    "            scores.append(mod.layer[1].EncDecAttention.score_storage)\n",
    "        scores = torch.cat(scores, dim=2)\n",
    "        bsz, n_heads, n_layers, _ = scores.size()\n",
    "        # batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "        scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n",
    "        scores = scores.masked_fill(~context_mask[:, None, None], 0.)\n",
    "        scores = scores.sum(dim=[1, 2, 4])\n",
    "        ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads\n",
    "        scores = scores/ntokens\n",
    "        return scores\n",
    "\n",
    "    def overwrite_forward_crossattention(self):\n",
    "        \"\"\"\n",
    "        Replace cross-attention forward function, only used to save\n",
    "        cross-attention scores.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            attn = mod.layer[1].EncDecAttention\n",
    "            attn.forward = types.MethodType(cross_attention_forward, attn)\n",
    "\n",
    "class EncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.main_input_name = \"input_ids\"\n",
    "        # self.linear = nn.Linear(896, 768)  # 用于调整拼接后的 hidden state 大小\n",
    "        self.encoder = encoder\n",
    "        self.gnn_out = None\n",
    "        apply_checkpoint_wrapper(self.encoder, use_checkpoint)\n",
    "\n",
    "        # Query 和 Key 的线性变换，用于计算注意力权重\n",
    "        # self.query_layer = nn.Linear(768, 512)  # 用于对T5的hidden state进行投影\n",
    "        # self.key_layer = nn.Linear(768, 512)    # 用于对GNN的输出进行投影\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, ggnn_output=None, **kwargs):\n",
    "        # print(input_ids.size(),attention_mask.size())\n",
    "        outputs = self.encoder(input_ids[:,:512], attention_mask[:,:512], **kwargs)\n",
    "        # print('input_ids', input_ids.size())\n",
    "        if self.gnn_out is not None:\n",
    "            self.gnn_out = self.gnn_out.to('cuda:1')\n",
    "            encoder_hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_dim)\n",
    "            # print(encoder_hidden_states.size())\n",
    "            # ggnn_output 是 (batch_size, 768)，我们需要将其转换为 query，变为 (batch_size, 1, 768)\n",
    "\n",
    "            encoder_hidden_states = torch.cat((encoder_hidden_states, self.gnn_out), dim=1)  # 替换填充部分\n",
    "            # print(encoder_hidden_states.size())\n",
    "\n",
    "\n",
    "            # print('outputs.last_hidden_state',outputs.last_hidden_state)\n",
    "            outputs.last_hidden_state = encoder_hidden_states\n",
    "\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "class CheckpointWrapper(torch.nn.Module):\n",
    "    def __init__(self, module, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, position_bias, **kwargs):\n",
    "        if self.use_checkpoint and self.training:\n",
    "            kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "            def custom_forward(*inputs):\n",
    "                output = self.module(*inputs, **kwargs)\n",
    "                empty = torch.tensor(\n",
    "                    [],\n",
    "                    dtype=torch.float,\n",
    "                    device=output[0].device,\n",
    "                    requires_grad=True)\n",
    "                output = tuple(x if x is not None else empty for x in output)\n",
    "                return output\n",
    "\n",
    "            output = torch.utils.checkpoint.checkpoint(\n",
    "                custom_forward,\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                position_bias\n",
    "            )\n",
    "            output = tuple(x if x.size() != 0 else None for x in output)\n",
    "        else:\n",
    "            output = self.module(hidden_states, attention_mask, position_bias, **kwargs)\n",
    "        return output\n",
    "\n",
    "def apply_checkpoint_wrapper(t5stack, use_checkpoint):\n",
    "    block = []\n",
    "    for mod in t5stack.block:\n",
    "        wrapped_mod = CheckpointWrapper(mod, use_checkpoint)\n",
    "        block.append(wrapped_mod)\n",
    "    block = nn.ModuleList(block)\n",
    "    t5stack.block = block\n",
    "\n",
    "def cross_attention_forward(\n",
    "        self,\n",
    "        input,\n",
    "        mask=None,\n",
    "        kv=None,\n",
    "        position_bias=None,\n",
    "        past_key_value_state=None,\n",
    "        head_mask=None,\n",
    "        query_length=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "    assert(kv != None)\n",
    "    assert(head_mask == None)\n",
    "    assert(position_bias != None or self.has_relative_attention_bias)\n",
    "\n",
    "    bsz, qlen, dim = input.size()\n",
    "    n_heads, d_heads = self.n_heads, self.d_kv\n",
    "    klen = kv.size(1)\n",
    "\n",
    "    q = self.q(input).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "    if past_key_value_state == None:\n",
    "        k = self.k(kv).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "        v = self.v(kv).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "    else:\n",
    "        k, v = past_key_value_state\n",
    "\n",
    "    scores = torch.einsum(\"bnqd,bnkd->bnqk\", q, k)\n",
    "\n",
    "    if mask is not None:\n",
    "       scores += mask\n",
    "\n",
    "    if position_bias is None:\n",
    "        position_bias = self.compute_bias(qlen, klen)\n",
    "    scores += position_bias\n",
    "\n",
    "    if self.score_storage is None:\n",
    "        self.score_storage = scores\n",
    "\n",
    "    attn = F.softmax(scores.float(), dim=-1).type_as(scores)\n",
    "    attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "\n",
    "    output = torch.matmul(attn, v)\n",
    "    output = output.transpose(1, 2).contiguous().view(bsz, -1, self.inner_dim)\n",
    "    output = self.o(output)\n",
    "\n",
    "    if use_cache:\n",
    "        output = (output,) + ((k, v),)\n",
    "    else:\n",
    "        output = (output,) + (None,)\n",
    "\n",
    "    if output_attentions:\n",
    "        output = output + (attn,)\n",
    "\n",
    "    if self.has_relative_attention_bias:\n",
    "        output = output + (position_bias,)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import dgl\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '/home/hdd/qingao/cache/huggingface/transformers/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c'\n",
    "    )\n",
    "tokenizer.add_tokens([\"Vul_Start\",\"Vul_End\"])\n",
    "model = transformers.T5ForConditionalGeneration.from_pretrained(\n",
    "    '/home/hdd/qingao/cache/huggingface/transformers/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c'\n",
    "    )\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(torch.load('/home/hdd/qingao/DeepDFA/CodeT5/saved_models/repair/codeT5/checkpoint-best-acc/pytorch_model.bin'))\n",
    "config = model.config\n",
    "\n",
    "\n",
    "input_dim = 8\n",
    "feat = \"_ABS_DATAFLOW_datatype_all_limitall_1000_limitsubkeys_1000\"\n",
    "gtype = \"cfg\"\n",
    "label_style = \"graph\"\n",
    "dsname = \"bigvul\"\n",
    "node_type_feat = None\n",
    "concat_all_absdf = True\n",
    "hidden_dim = 64\n",
    "n_steps = 5\n",
    "num_output_layers = 3\n",
    "\n",
    "flowgnn_model = FlowGNNGGNNModule(\n",
    "    feat,\n",
    "    input_dim,\n",
    "    hidden_dim,\n",
    "    n_steps,\n",
    "    num_output_layers,\n",
    "    label_style=label_style,\n",
    "    # freeze_graph=False,\n",
    "    # append_dataflow=\"before_graph\",\n",
    "    # codebert_feat=None,\n",
    "    # doc2vec_feat=None,\n",
    "    # glove_feat=None,\n",
    "    # num_node_types=flowgnn_datamodule.num_node_types,\n",
    "    # node_type_feat=node_type_feat,\n",
    "    # just_codebert=False,\n",
    "    concat_all_absdf=concat_all_absdf,\n",
    "    # undersample_node_on_loss_factor=None,\n",
    "    # test_every=False,\n",
    "    # tune_nni=False,\n",
    "    # positive_weight=None,\n",
    "    encoder_mode=True,\n",
    ")\n",
    "\n",
    "\n",
    "flow_model = FLOWT5(config,flow_gnn=flowgnn_model)\n",
    "flow_model.load_t5(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0] Train loss 2.684:  13%|█▎        | 54/410 [00:47<05:11,  1.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     graphs \u001b[38;5;241m=\u001b[39m [graphs_by_id[i]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m index_list \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m graphs_by_id]\n\u001b[0;32m---> 86\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mflow_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraphs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraphs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     91\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 42\u001b[0m, in \u001b[0;36mFLOWT5.forward\u001b[0;34m(self, input_ids, attention_mask, graph, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m graph:\n\u001b[0;32m---> 42\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ggnn_output 的形状为 (batch_size,seq, 512)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m<\u001b[39mpadding_length:\n\u001b[1;32m     44\u001b[0m         ggnn_output[i, :out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m=\u001b[39m out \n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 91\u001b[0m, in \u001b[0;36mFlowGNNGGNNModule.forward\u001b[0;34m(self, graph, extrafeats)\u001b[0m\n\u001b[1;32m     88\u001b[0m     feat_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(feat)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Graph learning stage (GGNN)\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m ggnn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mggnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# print(\"GGNN Output Size:\", ggnn_out.size())\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Token-level aggregation (reduce sequence dimension)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# 这里假设 code_embeddings 是 [node_num, seq_len, hid_dim]\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# 我们需要将其转换为适合 GRU 的形状\u001b[39;00m\n\u001b[1;32m     97\u001b[0m code_embeddings \u001b[38;5;241m=\u001b[39m code_embeddings\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, code_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_embedding_dim)  \u001b[38;5;66;03m# [node_num, seq_len, hid_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/dgl/nn/pytorch/conv/gatedgraphconv.py:172\u001b[0m, in \u001b[0;36mGatedGraphConv.forward\u001b[0;34m(self, graph, feat, etypes)\u001b[0m\n\u001b[1;32m    170\u001b[0m         graph\u001b[38;5;241m.\u001b[39mupdate_all(fn\u001b[38;5;241m.\u001b[39mcopy_e(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW_e*h\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m), fn\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    171\u001b[0m         a \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mndata\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# (N, D)\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feat\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1477\u001b[0m, in \u001b[0;36mGRUCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1475\u001b[0m     hx \u001b[38;5;241m=\u001b[39m hx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[0;32m-> 1477\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1484\u001b[0m     ret \u001b[38;5;241m=\u001b[39m ret\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import os \n",
    "# # 冻结 CodeT5 的参数\n",
    "# for param in flow_model.shared.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in flow_model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 如果 GNN 部分是单独的模块，确保它的参数不被冻结\n",
    "# # 这里假设 GNN 部分的名称为 `gnn`，请根据实际情况修改\n",
    "# for param in flow_model.flow_gnn.parameters():\n",
    "#     param.requires_grad = False  # 确保 GNN 的参数可训练\n",
    "\n",
    "\n",
    "# for param in flow_model.decoder.parameters():  # 确保流入解码器的参数被冻结\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # 定义学习率和优化器\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "best_em = 0\n",
    "# 冻结 CodeT5 encoder 和 decoder 的参数\n",
    "output_dir = 'home/hdd/qingao/RMGArepair/saved_model/step5_200nodes_encoder_decoder_freeze_gnn/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for param in flow_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flow_model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义不进行权重衰减的参数\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 为优化器分组参数\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "     'weight_decay': 1e-4\n",
    "     },\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad], \n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 定义 AdamW 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "# for name, param in flow_model.flow_gnn.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=400,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps=args.warmup_steps,\n",
    "#                                             num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "# 训练循环\n",
    "for cur_epoch in range(0, num_epochs):\n",
    "    bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "    nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "    flow_model.to('cuda:1')\n",
    "    flow_model.train()\n",
    "    \n",
    "    for step, batch in enumerate(bar):\n",
    "        input_ids = batch['input_ids'].to('cuda:1')\n",
    "        attention_mask = batch['attention_mask'].to('cuda:1')\n",
    "        labels = batch['labels'].to('cuda:1')\n",
    "        index = batch['index'].to('cuda:1')\n",
    "\n",
    "        index_list = index.tolist()\n",
    "\n",
    "        if graphs_by_id is None:\n",
    "            graphs = None\n",
    "        else:\n",
    "            graphs = [graphs_by_id[i].to('cuda:1') for i in index_list if i in graphs_by_id]\n",
    "\n",
    "            outputs = flow_model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                 labels=labels,\n",
    "                                 graph=graphs if graphs else None)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        loss.backward()\n",
    "\n",
    "        if nb_tr_steps % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Flow GNN Parameters and Gradients:\")\n",
    "            # for name, param in flow_model.flow_gnn.named_parameters():\n",
    "            #     if param.requires_grad:\n",
    "            #         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "            # break\n",
    "            # 计算和打印平均损失\n",
    "            train_loss = round(tr_loss / nb_tr_steps, 4) if nb_tr_steps > 0 else 0\n",
    "            bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "    em = eval(flow_model,eval_dataloader,graphs_by_id)\n",
    "    print(em)\n",
    "    if em>best_em:\n",
    "        best_em = em\n",
    "        output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "        torch.save(flow_model.state_dict(), output_model_file)\n",
    "        print(\"Save the best acc model into %s\", output_model_file)\n",
    "\n",
    "for param in flow_model.flow_gnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "for param in flow_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flow_model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义不进行权重衰减的参数\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 为优化器分组参数\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "     'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad], \n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "lr = 1e-4\n",
    "# 定义 AdamW 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "# for name, param in flow_model.flow_gnn.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=400,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps=args.warmup_steps,\n",
    "#                                             num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "# 训练循环\n",
    "for cur_epoch in range(0, 10):\n",
    "    bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "    nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "    flow_model.to('cuda:1')\n",
    "    flow_model.train()\n",
    "    \n",
    "    for step, batch in enumerate(bar):\n",
    "        input_ids = batch['input_ids'].to('cuda:1')\n",
    "        attention_mask = batch['attention_mask'].to('cuda:1')\n",
    "        labels = batch['labels'].to('cuda:1')\n",
    "        index = batch['index'].to('cuda:1')\n",
    "\n",
    "        index_list = index.tolist()\n",
    "\n",
    "        if graphs_by_id is None:\n",
    "            graphs = None\n",
    "        else:\n",
    "            graphs = [graphs_by_id[i].to('cuda:1') for i in index_list if i in graphs_by_id]\n",
    "\n",
    "            outputs = flow_model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                 labels=labels,\n",
    "                                 graph=graphs if graphs else None)\n",
    "            \n",
    "\n",
    "\n",
    "        loss = outputs.loss\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        loss.backward()\n",
    "\n",
    "        if nb_tr_steps % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Flow GNN Parameters and Gradients:\")\n",
    "            # for name, param in flow_model.flow_gnn.named_parameters():\n",
    "            #     if param.requires_grad:\n",
    "            #         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "            # break\n",
    "            # 计算和打印平均损失\n",
    "            train_loss = round(tr_loss / nb_tr_steps, 4) if nb_tr_steps > 0 else 0\n",
    "            bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "    em = eval(flow_model,eval_dataloader,graphs_by_id)\n",
    "    print(em)\n",
    "    if em>best_em:\n",
    "        best_em = em\n",
    "        output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "        torch.save(flow_model.state_dict(), output_model_file)\n",
    "        print(\"Save the best acc model into %s\", output_model_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2918192/252087155.py:349: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/hdd/qingao/DeepDFA/CodeT5/saved_models/repair/codeT5/checkpoint-best-acc/pytorch_model.bin'))\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/l1/qingao/DeepDFA/CodeT5')\n",
    "from modeling_t5 import T5ForConditionalGeneration\n",
    "\n",
    "class FLOWT5(T5ForConditionalGeneration):\n",
    "    def __init__(self, config,flow_gnn):\n",
    "        super().__init__(config)\n",
    "        self.flow_gnn = flow_gnn\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def forward_(self, **kwargs):\n",
    "        if 'input_ids' in kwargs:\n",
    "            kwargs['input_ids'] = kwargs['input_ids'].view(kwargs['input_ids'].size(0), -1)\n",
    "        if 'attention_mask' in kwargs:\n",
    "            kwargs['attention_mask'] = kwargs['attention_mask'].view(kwargs['attention_mask'].size(0), -1)\n",
    "\n",
    "        return super(FLOWT5, self).forward(\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # We need to resize as B x (N * L) instead of (B * N) x L here\n",
    "    # because the T5 forward method uses the input tensors to infer\n",
    "    # dimensions used in the decoder.\n",
    "    # EncoderWrapper resizes the inputs as (B * N) x L.\n",
    "    def forward(self, input_ids=None, attention_mask=None, graph=None, **kwargs):\n",
    "    # 获取 ggnn_output\n",
    "        ggnn_output = None\n",
    "        padding_length = 200\n",
    "        if graph is not None:\n",
    "            ggnn_output = torch.zeros(input_ids.size(0), padding_length, 768)\n",
    "            num_nodes = []\n",
    "            i = 0\n",
    "            for g in graph:\n",
    "                \n",
    "                out = self.flow_gnn(g, {})  # ggnn_output 的形状为 (batch_size,seq, 512)\n",
    "                if out.size(0)<padding_length:\n",
    "                    ggnn_output[i, :out.size(0)] = out \n",
    "                else:\n",
    "                    ggnn_output[i, :, :] = out[:padding_length,:] \n",
    "                num_nodes.append(out.size(0))\n",
    "                i += 1\n",
    "        self.encoder.gnn_out = ggnn_output\n",
    "\n",
    "        # print(ggnn_output.size())\n",
    "        # 保证 input_ids 是 2D tensor，确保其类型为 LongTensor\n",
    "        if input_ids is not None:\n",
    "            # 如果 input_ids 是 3D 维度，需要展平为 2D\n",
    "            if input_ids.dim() == 3:\n",
    "                self.encoder.n_passages = input_ids.size(1)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1).long()  # 确保 input_ids 是 LongTensor\n",
    "            if ggnn_output is not None:\n",
    "                padding = torch.zeros(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "\n",
    "        # 在最后一个维度上拼接 input_ids 和 padding\n",
    "    \n",
    "                input_ids = torch.cat((input_ids, padding), dim=1)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.view(attention_mask.size(0), -1)  # 确保 attention_mask 也是 2D\n",
    "            if ggnn_output is not None:\n",
    "                        # 创建与 input_ids 相同大小的填充\n",
    "                padding = torch.ones(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "                for i, num in enumerate(num_nodes):\n",
    "                    # 将前 num 个元素设置为 1\n",
    "                    padding[i, :num] = 1\n",
    "                # 在最后一个维度上拼接 attention_mask 和 padding\n",
    "                attention_mask = torch.cat((attention_mask, padding), dim=1)\n",
    "\n",
    "        # 将 ggnn_output 通过 kwargs 传递到 EncoderWrapper 中处理\n",
    "        # print('input_ids',input_ids.size())\n",
    "        # print('attention_mask',attention_mask.size())\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, # 将 ggnn_output 传入到 EncoderWrapper 进行后续处理\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # We need to resize the inputs here, as the generate method expect 2D tensors\n",
    "    def generate(self, input_ids, attention_mask, max_length, graph,**kwargs):\n",
    "        self.encoder.n_passages = input_ids.size(1)\n",
    "        ggnn_output = None\n",
    "        padding_length = 200\n",
    "        if graph is not None:\n",
    "            ggnn_output = torch.zeros(input_ids.size(0), padding_length, 768)\n",
    "            num_nodes = []\n",
    "            i = 0\n",
    "            for g in graph:\n",
    "                \n",
    "                out = self.flow_gnn(g, {})  # ggnn_output 的形状为 (batch_size,seq, 512)\n",
    "                if out.size(0)<padding_length:\n",
    "                    ggnn_output[i, :out.size(0)] = out \n",
    "                else:\n",
    "                    ggnn_output[i, :, :] = out[:padding_length,:] \n",
    "                num_nodes.append(out.size(0))\n",
    "                i += 1\n",
    "        self.encoder.gnn_out = ggnn_output\n",
    "        # 保证 input_ids 是 2D tensor，确保其类型为 LongTensor\n",
    "        if input_ids is not None:\n",
    "            # 如果 input_ids 是 3D 维度，需要展平为 2D\n",
    "            if input_ids.dim() == 3:\n",
    "                self.encoder.n_passages = input_ids.size(1)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1).long()  # 确保 input_ids 是 LongTensor\n",
    "            if ggnn_output is not None:\n",
    "                padding = torch.zeros(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "\n",
    "        # 在最后一个维度上拼接 input_ids 和 padding\n",
    "    \n",
    "                input_ids = torch.cat((input_ids, padding), dim=1)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.view(attention_mask.size(0), -1)  # 确保 attention_mask 也是 2D\n",
    "            if ggnn_output is not None:\n",
    "                        # 创建与 input_ids 相同大小的填充\n",
    "                padding = torch.ones(input_ids.size(0), padding_length, dtype=input_ids.dtype, device=input_ids.device)  # (batch_size, num_nodes)\n",
    "                for i, num in enumerate(num_nodes):\n",
    "                    # 将前 num 个元素设置为 1\n",
    "                    padding[i, :num] = 1\n",
    "                # 在最后一个维度上拼接 attention_mask 和 padding\n",
    "                attention_mask = torch.cat((attention_mask, padding), dim=1)\n",
    "        \n",
    "            # 将 ggnn_output 添加到 kwargs 中，以便在编码器中使用\n",
    "        return super().generate(\n",
    "            input_ids=input_ids.view(input_ids.size(0), -1),\n",
    "            attention_mask=attention_mask.view(attention_mask.size(0), -1),\n",
    "            max_length=max_length,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def wrap_encoder(self, use_checkpoint=False):\n",
    "        \"\"\"\n",
    "        Wrap T5 encoder to obtain a Fusion-in-Decoder model.\n",
    "        \"\"\"\n",
    "        self.encoder = EncoderWrapper(self.encoder, use_checkpoint=use_checkpoint)\n",
    "\n",
    "    def unwrap_encoder(self):\n",
    "        \"\"\"\n",
    "        Unwrap Fusion-in-Decoder encoder, useful to load T5 weights.\n",
    "        \"\"\"\n",
    "        self.encoder = self.encoder.encoder\n",
    "        block = []\n",
    "        for mod in self.encoder.block:\n",
    "            block.append(mod.module)\n",
    "        block = nn.ModuleList(block)\n",
    "        self.encoder.block = block\n",
    "\n",
    "    def load_t5(self, state_dict):\n",
    "        self.unwrap_encoder()\n",
    "        self.load_state_dict(state_dict,strict=False)\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def set_checkpoint(self, use_checkpoint):\n",
    "        \"\"\"\n",
    "        Enable or disable checkpointing in the encoder.\n",
    "        See https://pytorch.org/docs/stable/checkpoint.html\n",
    "        \"\"\"\n",
    "        for mod in self.encoder.encoder.block:\n",
    "            mod.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def reset_score_storage(self):\n",
    "        \"\"\"\n",
    "        Reset score storage, only used when cross-attention scores are saved\n",
    "        to train a retriever.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            mod.layer[1].EncDecAttention.score_storage = None\n",
    "\n",
    "    def get_crossattention_scores(self, context_mask):\n",
    "        \"\"\"\n",
    "        Cross-attention scores are aggregated to obtain a single scalar per\n",
    "        passage. This scalar can be seen as a similarity score between the\n",
    "        question and the input passage. It is obtained by averaging the\n",
    "        cross-attention scores obtained on the first decoded token over heads,\n",
    "        layers, and tokens of the input passage.\n",
    "\n",
    "        More details in Distilling Knowledge from Reader to Retriever:\n",
    "        https://arxiv.org/abs/2012.04584.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        n_passages = context_mask.size(1)\n",
    "        for mod in self.decoder.block:\n",
    "            scores.append(mod.layer[1].EncDecAttention.score_storage)\n",
    "        scores = torch.cat(scores, dim=2)\n",
    "        bsz, n_heads, n_layers, _ = scores.size()\n",
    "        # batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "        scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n",
    "        scores = scores.masked_fill(~context_mask[:, None, None], 0.)\n",
    "        scores = scores.sum(dim=[1, 2, 4])\n",
    "        ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads\n",
    "        scores = scores/ntokens\n",
    "        return scores\n",
    "\n",
    "    def overwrite_forward_crossattention(self):\n",
    "        \"\"\"\n",
    "        Replace cross-attention forward function, only used to save\n",
    "        cross-attention scores.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            attn = mod.layer[1].EncDecAttention\n",
    "            attn.forward = types.MethodType(cross_attention_forward, attn)\n",
    "\n",
    "class EncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.main_input_name = \"input_ids\"\n",
    "        # self.linear = nn.Linear(896, 768)  # 用于调整拼接后的 hidden state 大小\n",
    "        self.encoder = encoder\n",
    "        self.gnn_out = None\n",
    "        apply_checkpoint_wrapper(self.encoder, use_checkpoint)\n",
    "\n",
    "        # Query 和 Key 的线性变换，用于计算注意力权重\n",
    "        # self.query_layer = nn.Linear(768, 512)  # 用于对T5的hidden state进行投影\n",
    "        # self.key_layer = nn.Linear(768, 512)    # 用于对GNN的输出进行投影\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, ggnn_output=None, **kwargs):\n",
    "        # print(input_ids.size(),attention_mask.size())\n",
    "        outputs = self.encoder(input_ids[:,:512], attention_mask[:,:512], **kwargs)\n",
    "        # print('input_ids', input_ids.size())\n",
    "        if self.gnn_out is not None:\n",
    "            self.gnn_out = self.gnn_out.to('cuda:1')\n",
    "            encoder_hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_dim)\n",
    "            # print(encoder_hidden_states.size())\n",
    "            # ggnn_output 是 (batch_size, 768)，我们需要将其转换为 query，变为 (batch_size, 1, 768)\n",
    "\n",
    "            encoder_hidden_states = torch.cat((encoder_hidden_states, self.gnn_out), dim=1)  # 替换填充部分\n",
    "            # print(encoder_hidden_states.size())\n",
    "\n",
    "\n",
    "            # print('outputs.last_hidden_state',outputs.last_hidden_state)\n",
    "            outputs.last_hidden_state = encoder_hidden_states\n",
    "\n",
    "        # print(outputs)\n",
    "        return outputs\n",
    "class CheckpointWrapper(torch.nn.Module):\n",
    "    def __init__(self, module, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, position_bias, **kwargs):\n",
    "        if self.use_checkpoint and self.training:\n",
    "            kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "            def custom_forward(*inputs):\n",
    "                output = self.module(*inputs, **kwargs)\n",
    "                empty = torch.tensor(\n",
    "                    [],\n",
    "                    dtype=torch.float,\n",
    "                    device=output[0].device,\n",
    "                    requires_grad=True)\n",
    "                output = tuple(x if x is not None else empty for x in output)\n",
    "                return output\n",
    "\n",
    "            output = torch.utils.checkpoint.checkpoint(\n",
    "                custom_forward,\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                position_bias\n",
    "            )\n",
    "            output = tuple(x if x.size() != 0 else None for x in output)\n",
    "        else:\n",
    "            output = self.module(hidden_states, attention_mask, position_bias, **kwargs)\n",
    "        return output\n",
    "\n",
    "def apply_checkpoint_wrapper(t5stack, use_checkpoint):\n",
    "    block = []\n",
    "    for mod in t5stack.block:\n",
    "        wrapped_mod = CheckpointWrapper(mod, use_checkpoint)\n",
    "        block.append(wrapped_mod)\n",
    "    block = nn.ModuleList(block)\n",
    "    t5stack.block = block\n",
    "\n",
    "def cross_attention_forward(\n",
    "        self,\n",
    "        input,\n",
    "        mask=None,\n",
    "        kv=None,\n",
    "        position_bias=None,\n",
    "        past_key_value_state=None,\n",
    "        head_mask=None,\n",
    "        query_length=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "    assert(kv != None)\n",
    "    assert(head_mask == None)\n",
    "    assert(position_bias != None or self.has_relative_attention_bias)\n",
    "\n",
    "    bsz, qlen, dim = input.size()\n",
    "    n_heads, d_heads = self.n_heads, self.d_kv\n",
    "    klen = kv.size(1)\n",
    "\n",
    "    q = self.q(input).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "    if past_key_value_state == None:\n",
    "        k = self.k(kv).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "        v = self.v(kv).view(bsz, -1, n_heads, d_heads).transpose(1, 2)\n",
    "    else:\n",
    "        k, v = past_key_value_state\n",
    "\n",
    "    scores = torch.einsum(\"bnqd,bnkd->bnqk\", q, k)\n",
    "\n",
    "    if mask is not None:\n",
    "       scores += mask\n",
    "\n",
    "    if position_bias is None:\n",
    "        position_bias = self.compute_bias(qlen, klen)\n",
    "    scores += position_bias\n",
    "\n",
    "    if self.score_storage is None:\n",
    "        self.score_storage = scores\n",
    "\n",
    "    attn = F.softmax(scores.float(), dim=-1).type_as(scores)\n",
    "    attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "\n",
    "    output = torch.matmul(attn, v)\n",
    "    output = output.transpose(1, 2).contiguous().view(bsz, -1, self.inner_dim)\n",
    "    output = self.o(output)\n",
    "\n",
    "    if use_cache:\n",
    "        output = (output,) + ((k, v),)\n",
    "    else:\n",
    "        output = (output,) + (None,)\n",
    "\n",
    "    if output_attentions:\n",
    "        output = output + (attn,)\n",
    "\n",
    "    if self.has_relative_attention_bias:\n",
    "        output = output + (position_bias,)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import dgl\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '/home/hdd/qingao/cache/huggingface/transformers/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c'\n",
    "    )\n",
    "tokenizer.add_tokens([\"Vul_Start\",\"Vul_End\"])\n",
    "model = transformers.T5ForConditionalGeneration.from_pretrained(\n",
    "    '/home/hdd/qingao/cache/huggingface/transformers/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c'\n",
    "    )\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(torch.load('/home/hdd/qingao/DeepDFA/CodeT5/saved_models/repair/codeT5/checkpoint-best-acc/pytorch_model.bin'))\n",
    "config = model.config\n",
    "\n",
    "\n",
    "input_dim = 8\n",
    "feat = \"_ABS_DATAFLOW_datatype_all_limitall_1000_limitsubkeys_1000\"\n",
    "gtype = \"cfg\"\n",
    "label_style = \"graph\"\n",
    "dsname = \"bigvul\"\n",
    "node_type_feat = None\n",
    "concat_all_absdf = True\n",
    "hidden_dim = 64\n",
    "n_steps = 3\n",
    "num_output_layers = 3\n",
    "\n",
    "flowgnn_model = FlowGNNGGNNModule(\n",
    "    feat,\n",
    "    input_dim,\n",
    "    hidden_dim,\n",
    "    n_steps,\n",
    "    num_output_layers,\n",
    "    label_style=label_style,\n",
    "    # freeze_graph=False,\n",
    "    # append_dataflow=\"before_graph\",\n",
    "    # codebert_feat=None,\n",
    "    # doc2vec_feat=None,\n",
    "    # glove_feat=None,\n",
    "    # num_node_types=flowgnn_datamodule.num_node_types,\n",
    "    # node_type_feat=node_type_feat,\n",
    "    # just_codebert=False,\n",
    "    concat_all_absdf=concat_all_absdf,\n",
    "    # undersample_node_on_loss_factor=None,\n",
    "    # test_every=False,\n",
    "    # tune_nni=False,\n",
    "    # positive_weight=None,\n",
    "    encoder_mode=True,\n",
    ")\n",
    "\n",
    "\n",
    "flow_model = FLOWT5(config,flow_gnn=flowgnn_model)\n",
    "flow_model.load_t5(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0] Train loss 0.087:  38%|███▊      | 154/410 [02:05<03:28,  1.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m nb_tr_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m nb_tr_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 95\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nb_tr_steps \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import os \n",
    "# # 冻结 CodeT5 的参数\n",
    "# for param in flow_model.shared.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in flow_model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 如果 GNN 部分是单独的模块，确保它的参数不被冻结\n",
    "# # 这里假设 GNN 部分的名称为 `gnn`，请根据实际情况修改\n",
    "# for param in flow_model.flow_gnn.parameters():\n",
    "#     param.requires_grad = False  # 确保 GNN 的参数可训练\n",
    "\n",
    "\n",
    "# for param in flow_model.decoder.parameters():  # 确保流入解码器的参数被冻结\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # 定义学习率和优化器\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 2e-5\n",
    "num_epochs = 5\n",
    "best_em = 0\n",
    "# 冻结 CodeT5 encoder 和 decoder 的参数\n",
    "output_dir = 'home/hdd/qingao/RMGArepair/saved_model/step3_encoder_decoder_/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for param in flow_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in flow_model.decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 定义不进行权重衰减的参数\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 为优化器分组参数\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "     'weight_decay': 0.0\n",
    "     },\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad], \n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 定义 AdamW 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "# for name, param in flow_model.flow_gnn.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=400,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps=args.warmup_steps,\n",
    "#                                             num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "# 训练循环\n",
    "for cur_epoch in range(0, num_epochs):\n",
    "    bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "    nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "    flow_model.to('cuda:1')\n",
    "    flow_model.train()\n",
    "    \n",
    "    for step, batch in enumerate(bar):\n",
    "        input_ids = batch['input_ids'].to('cuda:1')\n",
    "        attention_mask = batch['attention_mask'].to('cuda:1')\n",
    "        labels = batch['labels'].to('cuda:1')\n",
    "        index = batch['index'].to('cuda:1')\n",
    "\n",
    "        index_list = index.tolist()\n",
    "\n",
    "        if graphs_by_id is None:\n",
    "            graphs = None\n",
    "        else:\n",
    "            graphs = [graphs_by_id[i].to('cuda:1') for i in index_list if i in graphs_by_id]\n",
    "\n",
    "            outputs = flow_model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                 labels=labels,\n",
    "                                 graph=graphs if graphs else None)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        loss.backward()\n",
    "\n",
    "        if nb_tr_steps % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Flow GNN Parameters and Gradients:\")\n",
    "            # for name, param in flow_model.flow_gnn.named_parameters():\n",
    "            #     if param.requires_grad:\n",
    "            #         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "            # break\n",
    "            # 计算和打印平均损失\n",
    "            train_loss = round(tr_loss / nb_tr_steps, 4) if nb_tr_steps > 0 else 0\n",
    "            bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "    em = eval(flow_model,eval_dataloader,graphs_by_id)\n",
    "    print(em)\n",
    "    if em>best_em:\n",
    "        best_em = em\n",
    "        output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "        torch.save(flow_model.state_dict(), output_model_file)\n",
    "        print(\"Save the best acc model into %s\", output_model_file)\n",
    "\n",
    "\n",
    "for param in flow_model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in flow_model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义不进行权重衰减的参数\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 为优化器分组参数\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "     'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in flow_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad], \n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 定义 AdamW 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "# for name, param in flow_model.flow_gnn.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=400,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps=args.warmup_steps,\n",
    "#                                             num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "# 训练循环\n",
    "for cur_epoch in range(0, 10):\n",
    "    bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "    nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "    flow_model.to('cuda:1')\n",
    "    flow_model.train()\n",
    "    \n",
    "    for step, batch in enumerate(bar):\n",
    "        input_ids = batch['input_ids'].to('cuda:1')\n",
    "        attention_mask = batch['attention_mask'].to('cuda:1')\n",
    "        labels = batch['labels'].to('cuda:1')\n",
    "        index = batch['index'].to('cuda:1')\n",
    "\n",
    "        index_list = index.tolist()\n",
    "\n",
    "        if graphs_by_id is None:\n",
    "            graphs = None\n",
    "        else:\n",
    "            graphs = [graphs_by_id[i].to('cuda:1') for i in index_list if i in graphs_by_id]\n",
    "\n",
    "            outputs = flow_model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                 labels=labels,\n",
    "                                 graph=graphs if graphs else None)\n",
    "            \n",
    "\n",
    "\n",
    "        loss = outputs.loss\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        loss.backward()\n",
    "\n",
    "        if nb_tr_steps % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Flow GNN Parameters and Gradients:\")\n",
    "            # for name, param in flow_model.flow_gnn.named_parameters():\n",
    "            #     if param.requires_grad:\n",
    "            #         print(f\"Parameter: {name}, Value: {param.data}, Gradient: {param.grad}\")\n",
    "            # break\n",
    "            # 计算和打印平均损失\n",
    "            train_loss = round(tr_loss / nb_tr_steps, 4) if nb_tr_steps > 0 else 0\n",
    "            bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "    em = eval(flow_model,eval_dataloader,graphs_by_id)\n",
    "    print(em)\n",
    "    if em>best_em:\n",
    "        best_em = em\n",
    "        output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "        torch.save(flow_model.state_dict(), output_model_file)\n",
    "        print(\"Save the best acc model into %s\", output_model_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "wandb_project = \"t5_gnn_tuning\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (0.34.2)\n",
      "Requirement already satisfied: torch in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from transformers[torch]) (1.12.0+cu116)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 3.5.0\n",
      "Uninstalling keras-3.5.0:\n",
      "  Would remove:\n",
      "    /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/keras-3.5.0.dist-info/*\n",
      "    /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/keras/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.8.1)\n",
      "Requirement already satisfied: namex in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall keras\n",
    "!pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1716557/3591218435.py\", line 48, in forward\n    return super().forward(\n  File \"/home/l1/qingao/DeepDFA/CodeT5/modeling_t5.py\", line 1752, in forward\n    decoder_outputs = self.decoder(\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/l1/qingao/DeepDFA/CodeT5/modeling_t5.py\", line 1021, in forward\n    raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\nValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:109\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    107\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 109\u001b[0m         output\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m    110\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1716557/3591218435.py\", line 48, in forward\n    return super().forward(\n  File \"/home/l1/qingao/DeepDFA/CodeT5/modeling_t5.py\", line 1752, in forward\n    decoder_outputs = self.decoder(\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/miniconda3/envs/deepdfa2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/l1/qingao/DeepDFA/CodeT5/modeling_t5.py\", line 1021, in forward\n    raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\nValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdfa2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
